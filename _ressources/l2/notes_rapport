# ____________________cache locality
Ref: https://stackoverflow.com/questions/19328909/nested-loop-optimisation-in-c-and-openmp
checking something like

int a[10000][10000];
for(int i = 0; i < 10000; i++){
    for(int j = 0; j < 10000; j++){
       a[i][j]++;               <-- good cache locality
       a[j][i]++;               <-- bad cache locality
    }
}

"In computer science, locality of reference, also known as the principle of locality,[1] is the tendency of a processor to access the same set of memory locations repetitively over a short period of time.[2] There are two basic types of reference locality – temporal and spatial locality. Temporal locality refers to the reuse of specific data, and/or resources, within a relatively small time duration. Spatial locality (also termed data locality[3]) refers to the use of data elements within relatively close storage locations. Sequential locality, a special case of spatial locality, occurs when data elements are arranged and accessed linearly, such as, traversing the elements in a one-dimensional array."

_____ CRITICAL vs ATOMIC
- CRITICAL can encapsulate a block where only one thread can enter at once, turning it into a sequential code inside a paralleized loop
- ATOMIC provides mutual exclusion but only to a specific memory location. It is generally faster than critical and applied to a single assignment that follows it

_____ GENERAL RULES
# DO NOT use omp for loops inside a FOR loop. Overheads have ^2 incurrance
1) use omp for inside a another parallel for
        #pragma omp parallel for
        for (int i = 0; i < 3; ++i) {
            #pragma omp for
            for (int j = 0; j < 6; ++j) {
                c(i, j);
            }
        }
2) parallel for twice outside and inside
        #pragma omp parallel for
        for (int i = 0; i < 3; ++i) {
            #pragma omp parallel for
            for (int j = 0; j < 6; ++j) {
                c(i, j);
            }
        }
3) use parallel for inside inner loop. REASON : In each iteration of outerloop, a parallel region is created causing overhead.
        for (int i = 0; i < 3; ++i) {
            #pragma omp parallel for
            for (int j = 0; j < 6; ++j) {
                c(i, j);
            }
        }
    
    sources : http://ppc.cs.aalto.fi/ch3/nested/

____ COLLAPSE 
- https://stackoverflow.com/questions/15160570/openmp-with-collapse-for-nested-for-loops-performs-worse-when-without
- can be used when we have perfectly nested loops

_____ ENCAPSULATE USING #pragma omp parallel
- http://ppc.cs.aalto.fi/ch3/for/

_____ PRIVATE/SHARED
- http://jakascorner.com/blog/2016/06/omp-data-sharing-attributes.html
- https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-11-01.pdf (slides with point form)
- if int is declared outside of #pragma omp parallel without specification, it is shared by default when parallelized. If internally declared, it is private.
–private (variable list)•Specifies variables local to each thread
–firstprivate(variable list)•Similar to the private•Private variables are initialized to variable value before the parallel directive
–shared (variable list)•Specifies variables that are shared among all the threads
–default (data scoping specifier)•Default data scoping specifiermay be shared or none

_____ SCHEDULING
: http://ppc.cs.aalto.fi/ch3/schedule/


_____ OBSERVATIONS
- on a 2x for perfect loops, collapse(2) is faster than schedule(static) because both loops are parallelized

_____ other good slides
- https://people.math.umass.edu/~johnston/PHI_WG_2014/OpenMPSlides_tamu_sc.pdf (good for dummies and definitions). AMDAHLS LAW
-https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-11-01.pdf lots of examples and explanations
- quebec reference?? https://wiki.calculquebec.ca/w/Parall%C3%A9lisation?setlang=en


=========FINAL NOTES FOR PROB 1

original code

    for (int k = 1; k <= iterations; k++)
        for (int j = 0; j < cols; j++)
            for (int i = 0; i < rows; i++)
                matrix[i][j] = matrix[i][j] + i + j;

1) first, we swapped i and j loops so that j would be internal for better cache locality (this is called Exchange in our class notes)
2) then, because k is a dependency on an external loop, we do another exchange to internalize it
3) we see an opportunity to eliminate k loop and integrate it into a single line, reducing our problem to 2 loops for better parallization
4) use collapse to paral the 2 for loops. each para instance only needs 1 instance of instruction. no race conditions.

=========FINAL NOTES FOR PROB 2

original code

    for (int k = 1; k <= iterations; k++)
	{
		for (int i = 0; i < rows; i++)
		{
			for (int j = cols - 1; j >= 0; j--)
			{
				if (j == cols - 1)
				{
					matrix[i][j] = matrix[i][j] + i;
				}
				else
				{
					matrix[i][j] = matrix[i][j] + matrix[i][j+1];
				}
			}
		}
	}

1) first, we inverted the flow of j from neg to pos as it is better for cache locality